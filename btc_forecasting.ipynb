{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "btc_forecasting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19jVQv3fX4_AwBgaCy9Pb4t20A7U9N33F",
      "authorship_tag": "ABX9TyM5uyjBP6UwnUMvNOmr/RpL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DPybvqkNR0N"
      },
      "source": [
        "Loading dependency packages.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPamtUvP6sKu"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.externals import joblib\r\n",
        "from sklearn.metrics import r2_score\r\n",
        "from sklearn import preprocessing\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "from tensorflow.keras import initializers\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.models import model_from_json\r\n",
        "from tensorflow.keras.metrics import *\r\n",
        "\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDdKsQHf9v3"
      },
      "source": [
        "Superparameters for tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0mZVDXmgJ2l"
      },
      "source": [
        "# Features\r\n",
        "processing_from = '2016-01-01'\r\n",
        "processing_to = '2020-12-31'\r\n",
        "label_columns = ['Weighted_Price']\r\n",
        "input_features = 2\r\n",
        "output_features = len(label_columns)\r\n",
        "Tx = 7\r\n",
        "Ty = 3\r\n",
        "shift = 1\r\n",
        "cost_function=\"mse\"\r\n",
        "optimizer=\"adam\"\r\n",
        "BATCH_SIZE = 128\r\n",
        "learning_rate = 0.0001\r\n",
        "num_epochs = 50\r\n",
        "L2_lambd = 0.0\r\n",
        "beta1 = 0.0\r\n",
        "beta2 = 0.0\r\n",
        "epsilon = 0.0\r\n",
        "LSTM_units = 512\r\n",
        "LSTM_dropout = 0.6\r\n",
        "dropout_rate = 0.6\r\n",
        "\r\n",
        "#INPUT_DATA\r\n",
        "IN_PATH = os.path.abspath('F:/Work/DATA/BTC/')\r\n",
        "trained_model_path = \"BTC_regressor_daily_v20210129.h5\"\r\n",
        "trained_model_figure_path = \"BTC_regressor_daily_v20210129.png\"\r\n",
        "dataset_scaler_path = \"20180101_20201231_daily.scl\"\r\n",
        "preprocessed_dataset_path = \"20180101_20201231_daily.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmHqopZ1UGXF"
      },
      "source": [
        "Define CSV reading function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUNtLKh5iDeS"
      },
      "source": [
        "# data reading\r\n",
        "def csv_reading(url, options):\r\n",
        "  if options == 0:\r\n",
        "    content = pd.read_csv(url)\r\n",
        "    return content\r\n",
        "  elif options == 1:\r\n",
        "    file_id = url.split('/')[-2]\r\n",
        "    path='https://drive.google.com/uc?export=download&id=' + file_id\r\n",
        "    file_name = 'bitstampUSD_1-min_data_2012-01-01_to_2020-12-31.csv'\r\n",
        "    auth.authenticate_user()\r\n",
        "    gauth = GoogleAuth()\r\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "    drive = GoogleDrive(gauth)\r\n",
        "    fileDownloaded = drive.CreateFile({'id':file_id})\r\n",
        "    fileDownloaded.GetContentFile(file_name)\r\n",
        "    content = pd.read_csv(file_name)\r\n",
        "    return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5H_uMTERAcd"
      },
      "source": [
        "Define preprocessing function to get daily trading data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaHqFClFRE39"
      },
      "source": [
        "def daily_preprocess(df):\r\n",
        "  price = df.resample('d', on='Timestamp')[['Weighted_Price']].mean()\r\n",
        "  volume_usd = df.resample('d', on='Timestamp')[['Volume_(Currency)']].sum()\r\n",
        "  close_price = df.resample('d', on='Timestamp')[['Close']].last('T')\r\n",
        "  fig, axs = plt.subplots(3, 1, figsize=(20,10))\r\n",
        "  fig.suptitle('Full BTC historical trading (daily)', fontsize= 18, color='b')\r\n",
        "  axs[0].plot(price, 'g')\r\n",
        "  axs[0].set_ylabel('BTC daily weighted price ($)')\r\n",
        "  axs[1].plot(close_price, 'b')\r\n",
        "  axs[1].set_ylabel('BTC daily close price ($)')\r\n",
        "  axs[2].plot(volume_usd, 'r')\r\n",
        "  axs[2].set_ylabel('BTC daily volume ($)')\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "  # removed NaN and slice data\r\n",
        "  price.fillna(method ='bfill', inplace = True)\r\n",
        "  close_price.fillna(method ='bfill', inplace = True)\r\n",
        "  volume_usd.fillna(method ='bfill', inplace = True)\r\n",
        "  price = price[processing_from:processing_to]\r\n",
        "  close_price = close_price[processing_from:processing_to]\r\n",
        "  volume_usd = volume_usd[processing_from:processing_to]\r\n",
        "  fig, axs = plt.subplots(3, 1, figsize=(20,10))\r\n",
        "  fig.suptitle('Sliced BTC historical trading (daily)', fontsize= 18, color='b')\r\n",
        "  axs[0].plot(price, 'g')\r\n",
        "  axs[0].set_ylabel('BTC daily weighted price ($)')\r\n",
        "  axs[1].plot(close_price, 'b')\r\n",
        "  axs[1].set_ylabel('BTC daily close price ($)')\r\n",
        "  axs[2].plot(volume_usd, 'r')\r\n",
        "  axs[2].set_ylabel('BTC volume (currency) ($)')\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])\r\n",
        "  plt.show()\r\n",
        "  daily = pd.concat([price, volume_usd], join = 'outer', axis = 1)\r\n",
        "  return daily"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBxDpppUawU"
      },
      "source": [
        "Loading dataset from Google drive and visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLTz7sR3UlgZ"
      },
      "source": [
        "dataset_url='https://drive.google.com/file/d/16AoK0vKAIshdyO96Td4Vfmnf_6cdTBko/view?usp=sharing'\r\n",
        "btc = csv_reading(dataset_url, 1)\r\n",
        "btc['Timestamp'] = pd.to_datetime(btc.Timestamp, unit='s')\r\n",
        "print(btc.info())\r\n",
        "daily = daily_preprocess(btc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhxMVZpumaxw"
      },
      "source": [
        "Define models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfOhXNsgmp5_"
      },
      "source": [
        "def BTC_forecasting_model(num_of_input_features, Tx, num_of_out_features, Ty, LSTM_units, LSTM_dropout, dropout_rate):\r\n",
        "  ''' v1\r\n",
        "  model = Sequential()\r\n",
        "  model.add(LSTM(LSTM_units, activation='relu', return_sequences=True, input_shape=(Tx, num_of_input_features), dropout=LSTM_dropout))\r\n",
        "  model.add(Dropout(rate=dropout_rate))\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  model.add(LSTM(LSTM_units, dropout=LSTM_dropout))\r\n",
        "  model.add(Dropout(rate=dropout_rate))\r\n",
        "  model.add(Dense(num_of_out_features, activation='sigmoid'))\r\n",
        "  '''\r\n",
        "  '''\r\n",
        "  model = Sequential()\r\n",
        "  model.add(LSTM(LSTM_units, activation='relu', return_sequences=True, input_shape=(Tx, num_of_input_features),dropout=LSTM_dropout))\r\n",
        "  #model.add(BatchNormalization())\r\n",
        "  model.add(LSTM(LSTM_units, activation='relu',dropout=LSTM_dropout))\r\n",
        "  model.add(Dropout(rate=dropout_rate)(training=True))\r\n",
        "  model.add(Dense(num_of_out_features))\r\n",
        "  '''\r\n",
        "\r\n",
        "  input_shape=(Tx, num_of_input_features)\r\n",
        "  X_input = Input(input_shape)\r\n",
        "  X = LSTM(LSTM_units, activation='relu', return_sequences=False, input_shape=input_shape, name='LSTM_1')(X_input)\r\n",
        "  #X = BatchNormalization(name='BN_1')(X)\r\n",
        "  #X = LSTM(LSTM_units, activation='relu',dropout=LSTM_dropout,name='LSTM_2')(X)\r\n",
        "  #X = Dropout(rate=dropout_rate,name='Dropout_1')(X, training=True)\r\n",
        "  X = Dense(num_of_out_features, activation='sigmoid',name='Full_1')(X)\r\n",
        "  model = Model(inputs = X_input, outputs = X, name='BTC_forecasting')\r\n",
        "\r\n",
        "  if optimizer == 'adam':\r\n",
        "    opt = optimizers.Adam(learning_rate=learning_rate)\r\n",
        "  elif optimizer == 'SGD':\r\n",
        "    opt = optimizers.SGD(learning_rate=learning_rate)\r\n",
        "  model.compile(optimizer=opt, loss=cost_function, metrics=[MeanSquaredError()])\r\n",
        "  return model\r\n",
        "\r\n",
        "def BTC_daily_forecasting_multi_steps_model(num_of_input_features, Tx, num_of_out_features, Ty, LSTM_units, LSTM_dropout, dropout_rate):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(LSTM(LSTM_units, activation='relu', input_shape=(Tx, num_of_input_features)))\r\n",
        "  #model.add(Dropout(rate=dropout_rate))\r\n",
        "  #model.add(BatchNormalization())\r\n",
        "  #model.add(LSTM(LSTM_units, dropout=LSTM_dropout))\r\n",
        "  model.add(Dense(Ty*num_of_out_features, activation='sigmoid'))\r\n",
        "  model.add(Reshape((Ty, num_of_out_features)))\r\n",
        "  \r\n",
        "  if optimizer == 'adam':\r\n",
        "    opt = optimizers.Adam(learning_rate=learning_rate)\r\n",
        "  elif optimizer == 'SGD':\r\n",
        "    opt = optimizers.SGD(learning_rate=learning_rate)\r\n",
        "  model.compile(optimizer=opt, loss=cost_function, metrics=[MeanSquaredError()])\r\n",
        "  return model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbg0_MOWRmp1"
      },
      "source": [
        "Define data generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNjr47bARqMy"
      },
      "source": [
        "class WindowGenerator():\r\n",
        "  def __init__(self, input_width, label_width, shift,\r\n",
        "               input_df,\r\n",
        "               label_columns=None):\r\n",
        "    # Store the raw data.\r\n",
        "    self.input_df = input_df\r\n",
        "\r\n",
        "    # Work out the label column indices.\r\n",
        "    self.label_columns = label_columns\r\n",
        "    if label_columns is not None:\r\n",
        "      self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\r\n",
        "    self.column_indices = {name: i for i, name in enumerate(input_df.columns)}\r\n",
        "\r\n",
        "    # Work out the window parameters.\r\n",
        "    self.input_width = input_width\r\n",
        "    self.label_width = label_width\r\n",
        "    self.shift = shift\r\n",
        "\r\n",
        "    self.total_window_size = input_width + shift\r\n",
        "\r\n",
        "    self.input_slice = slice(0, input_width)\r\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\r\n",
        "\r\n",
        "    self.label_start = self.total_window_size - self.label_width\r\n",
        "    self.labels_slice = slice(self.label_start, None)\r\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\r\n",
        "\r\n",
        "  def __repr__(self):\r\n",
        "    return '\\n'.join([\r\n",
        "        f'Total window size: {self.total_window_size}',\r\n",
        "        f'Input indices: {self.input_indices}',\r\n",
        "        f'Label indices: {self.label_indices}',\r\n",
        "        f'Label column name(s): {self.label_columns}'])\r\n",
        "  \r\n",
        "def sequence_data_generator(df, label_columns, input_len, output_len, shift, batch_size):\r\n",
        "  total_window_size = input_len + shift\r\n",
        "  inputs = np.array(df)\r\n",
        "  labels = np.array(df[label_columns])\r\n",
        "  x_shape = (batch_size, input_len, inputs.shape[1])\r\n",
        "  x_batch = np.zeros(shape=x_shape, dtype=np.float16)\r\n",
        "  y_shape = (batch_size, output_len, 1)\r\n",
        "  y_batch = np.zeros(shape=y_shape, dtype=np.float16)\r\n",
        "\r\n",
        "  while True:\r\n",
        "    for i in range(batch_size):\r\n",
        "      idx = np.random.randint(len(inputs) - total_window_size)\r\n",
        "      x_batch[i] = inputs[idx:idx+input_len]\r\n",
        "      y_batch[i] = labels[idx + total_window_size-output_len:idx + total_window_size]\r\n",
        "    yield (x_batch, y_batch)\r\n",
        "\r\n",
        "def convert_fit_data(df, label_columns, input_len, output_len, shift):\r\n",
        "    data_x = []\r\n",
        "    data_y = []\r\n",
        "    for i in range(len(df) - input_len - shift):\r\n",
        "        x_floats = np.array(df.iloc[i:i+input_len])\r\n",
        "        y_floats = np.array(df[label_columns].iloc[i+input_len+shift-output_len:i+input_len+shift])\r\n",
        "        data_x.append(x_floats)\r\n",
        "        data_y.append(y_floats)\r\n",
        "    return np.array(data_x), np.array(data_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA8z8W6dnOCj"
      },
      "source": [
        "Normalize data and split into train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMs-GbcVLw1f"
      },
      "source": [
        "columns_name = daily.columns\r\n",
        "scaler = preprocessing.MinMaxScaler()\r\n",
        "btc_scaled = daily.copy()\r\n",
        "print(btc_scaled.head())\r\n",
        "scaler = scaler.fit(daily)\r\n",
        "btc_scaled[columns_name] = scaler.transform(daily)\r\n",
        "print(btc_scaled.head())\r\n",
        "train, test = train_test_split(btc_scaled, test_size=0.2, random_state=1)\r\n",
        "train, validate = train_test_split(train, test_size=0.25, random_state=1)\r\n",
        "X,y = convert_fit_data(btc_scaled, label_columns, Tx, Ty, shift)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\r\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\r\n",
        "# save\r\n",
        "joblib.dump(scaler, dataset_scaler_path)\r\n",
        "saved_data = {'X_train':X_train,'y_train':y_train,'X_val':X_validate,'y_val':y_validate, 'X_test':X_test,'y_test':y_test}\r\n",
        "np.save(preprocessed_dataset_path, saved_data)\r\n",
        "#\r\n",
        "print('train size: ' + str(X_train.shape))\r\n",
        "print('train label size: ' + str(y_train.shape))\r\n",
        "print('validate size: ' + str(X_validate.shape))\r\n",
        "print('test size: ' + str(X_test.shape))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0pcxxSWF3U"
      },
      "source": [
        "Model, train and validate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDfQrwYTri1w"
      },
      "source": [
        "checkpoint = ModelCheckpoint(trained_model_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "#model = BTC_forecasting_model(input_features, Tx, output_features, Ty, LSTM_units, LSTM_dropout, dropout_rate)\r\n",
        "model = BTC_daily_forecasting_multi_steps_model(input_features, Tx, output_features, Ty, LSTM_units, LSTM_dropout, dropout_rate)\r\n",
        "model.summary()\r\n",
        "tf.keras.utils.plot_model(model, to_file=trained_model_figure_path, show_shapes=True, show_layer_names=True, expand_nested=False, dpi=96)\r\n",
        "#history = model.fit(sequence_data_generator(train, label_columns, Tx, Ty, shift, BATCH_SIZE),validation_data=sequence_data_generator(validate, label_columns, Tx, Ty, shift, BATCH_SIZE),validation_steps=len(validate)-Tx-shift, steps_per_epoch=len(train)-Tx-shift, epochs=num_epochs)\r\n",
        "history = model.fit(X_train, y_train,epochs=num_epochs, batch_size=BATCH_SIZE, callbacks=[checkpoint], shuffle=True, validation_data=(X_validate, y_validate))\r\n",
        "\r\n",
        "plt.plot(history.history['loss'], label='Training Loss')\r\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "# predict on train/test dataset\r\n",
        "#test_pred = model.predict(sequence_data_generator(test, label_columns, Tx, Ty, shift, len(test) - shift - Tx), steps=1)\r\n",
        "#test_true = np.array(test[['Weighted_Price']])[Tx+shift-Ty:,0]\r\n",
        "#train_pred = model.predict(sequence_data_generator(train, label_columns, Tx, Ty, shift, len(train) - shift - Tx), steps=1)\r\n",
        "#train_true = np.array(train[['Weighted_Price']])[Tx+shift-Ty:,0]\r\n",
        "\r\n",
        "test_true = np.copy(y_test)\r\n",
        "train_true = np.copy(y_train)\r\n",
        "test_pred = model.predict(X_test)\r\n",
        "test_true = test_true.reshape(test_pred.shape)\r\n",
        "train_pred = model.predict(X_train)\r\n",
        "train_true = train_true.reshape(train_pred.shape)\r\n",
        "train_mse = tf.keras.losses.MeanSquaredError()(train_true, train_pred).numpy()\r\n",
        "test_mse = tf.keras.losses.MeanSquaredError()(test_true, test_pred).numpy()\r\n",
        "# transform to original value\r\n",
        "scaler = joblib.load(dataset_scaler_path)\r\n",
        "inverse_scaler = preprocessing.MinMaxScaler()\r\n",
        "inverse_scaler.min_,inverse_scaler.scale_ = scaler.min_[0],scaler.scale_[0]\r\n",
        "X_train_org = np.copy(X_train)\r\n",
        "X_test_org = np.copy(X_test)\r\n",
        "\r\n",
        "for i in range(Tx):\r\n",
        "  X_train_org[:,i,:] = scaler.inverse_transform(X_train[:,i,:])\r\n",
        "  X_test_org[:,i,:] = scaler.inverse_transform(X_test[:,i,:])\r\n",
        "for i in range(Ty):\r\n",
        "  test_pred[:,i,:] = inverse_scaler.inverse_transform(test_pred[:,i,:])\r\n",
        "  test_true[:,i,:] = inverse_scaler.inverse_transform(test_true[:,i,:])\r\n",
        "  train_pred[:,i,:] = inverse_scaler.inverse_transform(train_pred[:,i,:])\r\n",
        "  train_true[:,i,:] = inverse_scaler.inverse_transform(train_true[:,i,:])\r\n",
        "\r\n",
        "test_r2 = r2_score(test_true[:,:,0], test_pred[:,:,0])\r\n",
        "train_r2 = r2_score(train_true[:,:,0], train_pred[:,:,0])\r\n",
        "#\r\n",
        "font = {'family': 'serif',\r\n",
        "        'color':  'darkred',\r\n",
        "        'weight': 'normal',\r\n",
        "        'size': 16,\r\n",
        "        }\r\n",
        "f = plt.figure(figsize=(20,10))\r\n",
        "ax = f.add_subplot()\r\n",
        "rand_idx = random.sample(range(len(y_test)), 3)\r\n",
        "for i in range(3):\r\n",
        "  # plt.plot(X_train_org[rand_idx[i],:,0], 'C{n}+-'.format(n=i), label='input of train sample {spl}'.format(spl=rand_idx[i]))\r\n",
        "  # plt.plot(np.arange(Ty)+Tx, train_true[rand_idx[i],:,:], 'C{n}o'.format(n=i), label='true output of train sample {spl}'.format(spl=rand_idx[i]))\r\n",
        "  # plt.plot(np.arange(Ty)+Tx, train_pred[rand_idx[i],:,:], 'C{n}o'.format(n=i), markerfacecolor='none', label='pred output of train sample {spl}'.format(spl=rand_idx[i]))\r\n",
        "\r\n",
        "  plt.plot(X_test_org[rand_idx[i],:,0], 'C{n}+-'.format(n=i), label='input of test sample {spl}'.format(spl=rand_idx[i]))\r\n",
        "  plt.plot(np.arange(Ty)+Tx, test_true[rand_idx[i],:,:], 'C{n}o'.format(n=i), label='true output of test sample {spl}'.format(spl=rand_idx[i]))\r\n",
        "  plt.plot(np.arange(Ty)+Tx, test_pred[rand_idx[i],:,:], 'C{n}o'.format(n=i), markerfacecolor='none', label='pred output of test sample {spl}'.format(spl=rand_idx[i]))\r\n",
        "\r\n",
        "\r\n",
        "plt.text(0.3, 0.95, \"train MSE = {mse}\".format(mse = round(train_mse, 5)), fontdict=font, transform=ax.transAxes)\r\n",
        "plt.text(0.3, 0.90, \"train R2 = {r2_score}\".format(r2_score = round(train_r2, 5)), fontdict=font, transform=ax.transAxes)\r\n",
        "plt.text(0.3, 0.85, \"test MSE = {mse}\".format(mse = round(test_mse, 5)), fontdict=font, transform=ax.transAxes)\r\n",
        "plt.text(0.3, 0.80, \"test R2 = {r2_score}\".format(r2_score = round(test_r2, 5)), fontdict=font, transform=ax.transAxes)\r\n",
        "plt.xlabel('Day')\r\n",
        "plt.ylabel('Weighted price($)')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vch8NXoNJ8BK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}